{"meta":{"title":"IBaby","subtitle":"小小爬虫","description":"啦啦啦，欢迎啊！","author":"汪辉","url":"http://yoursite.com","root":"/Ibaby.github.io/"},"pages":[],"posts":[{"title":"爬虫小作业/爬虫小作业","slug":"爬虫小作业/爬虫小作业","date":"2020-05-15T00:21:04.926Z","updated":"2020-05-15T00:19:44.430Z","comments":true,"path":"2020/05/15/爬虫小作业/爬虫小作业/","link":"","permalink":"http://yoursite.com/2020/05/15/%E7%88%AC%E8%99%AB%E5%B0%8F%E4%BD%9C%E4%B8%9A/%E7%88%AC%E8%99%AB%E5%B0%8F%E4%BD%9C%E4%B8%9A/","excerpt":"","text":"实战一1234567891011121314151617181920212223242526272829303132333435363738394041424344#作业就是读取到响应内容 然后提取出“发送成功”和发送的信息 提示：使用json模块能将字符串转换成字典from urllib import request,parseimport http.cookiejarimport gzipimport jsondef crawler(): message &#x3D; str(input(&#39;请输入你的评论:&#39;)) url &#x3D; &#39; https:&#x2F;&#x2F;api.bilibili.com&#x2F;x&#x2F;v2&#x2F;reply&#x2F;add&#39; form_data &#x3D; &#123; &#39;oid&#39;: &#39;540379650&#39;, &#39;type&#39;: &#39;1&#39;, &#39;message&#39;: message, &#39;plat&#39;: &#39;1&#39;, &#39;jsonp&#39;: &#39;jsonp&#39;, &#39;csrf&#39;: &#39;2465bf0f43863f1f72da4437e8a3f7f7&#39; &#125; data &#x3D; bytes(parse.urlencode(form_data),encoding&#x3D;&#39;utf-8&#39;) headers &#x3D; &#123; &quot;User-agent&quot;: &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;81.0.4044.129 Safari&#x2F;537.36&quot;, &quot;Host&quot;: &quot;api.bilibili.com&quot;, &quot;Content-Type&quot;: &quot;application&#x2F;x-www-form-urlencoded; charset&#x3D;UTF-8&quot;, &quot;Accept&quot;: &quot;application&#x2F;json, text&#x2F;javascript, *&#x2F;*; q&#x3D;0.01&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate, br&quot;, &quot;Accept-Language&quot;: &quot;zh-CN,zh;q&#x3D;0.9&quot;, &quot;Connection&quot;: &quot;keep-alive&quot;, &quot;Origin&quot;: &quot;https:&#x2F;&#x2F;www.bilibili.com&quot;, &quot;Referer&quot;: &quot;https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;BV1ii4y1t7Ri?from&#x3D;search&amp;seid&#x3D;13227842417220944957&quot;, &quot;Cookie&quot;:&quot;_uuid&#x3D;6545DCA1-06AD-4CC5-DAEE-66A47A079FB968566infoc; buvid3&#x3D;FB65770C-8357-41F6-B780-8B395463BE9253923infoc; sid&#x3D;lfvz8p4k; DedeUserID&#x3D;452320745; DedeUserID__ckMd5&#x3D;d305967df50bec7c; SESSDATA&#x3D;c722af0c%2C1602934275%2C4c34d*41; bili_jct&#x3D;2465bf0f43863f1f72da4437e8a3f7f7; CURRENT_FNVAL&#x3D;16; LIVE_BUVID&#x3D;AUTO1815873823108358; rpdid&#x3D;|(J|)R)|~R|J0J&#39;ul)~J)kJ|m; CURRENT_QUALITY&#x3D;64; bfe_id&#x3D;393becc67cde8e85697ff111d724b3c8; PVID&#x3D;1&quot; &#125; #构建requests对象 requests &#x3D; request.Request(url&#x3D;url,headers&#x3D;headers,data&#x3D;data) #CookieJar 类储存 HTTP cookies。它从 HTTP 请求提取 cookies，并在 HTTP 响应中返回它们。 cookie &#x3D; http.cookiejar.CookieJar() #request.HTTPCookieProcessor用于处理 HTTP Cookies handler &#x3D; request.HTTPCookieProcessor(cookie) #request.build_opener返回一个OpenerDirector实例，该实例按给定的顺序链接处理程序 opener &#x3D; request.build_opener(handler) response &#x3D; opener.open(requests) #GzipFile 类可以读写 gzip 格式的文件，还能自动压缩和解压缩数据,此处用的是gzip 格式，所以要调用次方法来读数据 response &#x3D; gzip.GzipFile(fileobj&#x3D;response) #转化成字典 response &#x3D; json.loads(response.read()) print(response[&#39;data&#39;][&#39;success_toast&#39;])crawler() 实战二 2.1123456789101112131415161718192021import requestsfrom lxml import etreedef crawler(): url &#x3D; &quot;http:&#x2F;&#x2F;www.stats.gov.cn&#x2F;tjsj&#x2F;tjbz&#x2F;tjyqhdmhcxhfdm&#x2F;2019&#x2F;index.html&quot; headers &#x3D; &#123; &quot;user-agent&quot;: &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.149 Safari&#x2F;537.36&quot; &#125; url2 &#x3D; &quot;http:&#x2F;&#x2F;www.stats.gov.cn&#x2F;tjsj&#x2F;tjbz&#x2F;tjyqhdmhcxhfdm&#x2F;2019&#x2F;&quot; response &#x3D; requests.get(url&#x3D;url,headers&#x3D;headers) response.encoding &#x3D; &quot;GB2312&quot; html &#x3D; etree.HTML(response.text) name_result &#x3D; html.xpath(&#39;&#x2F;&#x2F;a&#x2F;text()&#39;)[:31] herf_result &#x3D; html.xpath(&quot;&#x2F;&#x2F;a&#x2F;@href&quot;)[:31] result &#x3D; &#123;&#125; for i in range(len(name_result) ): result[name_result[i]] &#x3D; url2 + herf_result[i] print(result)crawler() 2.212345678910111213141516171819import requestsimport jsondef bingTranslate(): text &#x3D; input(&#39;请输入你要翻译的语句（中 -&gt; 英）：&#39;) headers &#x3D; &#123; &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 6.1; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;63.0.3239.84 Safari&#x2F;537.36&#39;, &#125; url &#x3D; &#39;https:&#x2F;&#x2F;cn.bing.com&#x2F;ttranslatev3?isVertical&#x3D;1&amp;&amp;IG&#x3D;07B681A73A8C4F059F058F12AC6279F5&amp;IID&#x3D;translator.5027.2&#39; data &#x3D; &#123; &#39;fromLang&#39;: &#39;auto-detect&#39;, &#39;text&#39;: text, &#39;to&#39;: &#39;en&#39; &#125; response &#x3D; requests.post(url&#x3D;url,data&#x3D;data,headers&#x3D;headers) response &#x3D; json.loads(response.text) # print(response) print(&#39;翻译结果为：&#39;,end&#x3D;&#39; &#39;) print(response[0][&#39;translations&#39;][0][&#39;text&#39;])bingTranslate() 2.312345678910111213141516171819202122232425262728293031323334# coding&#x3D;gbkimport requestsimport jsondef crawler(): message &#x3D; input(&#39;请输入你的评论:&#39;) url &#x3D; &#39; https:&#x2F;&#x2F;api.bilibili.com&#x2F;x&#x2F;v2&#x2F;reply&#x2F;add&#39; data &#x3D; &#123; &#39;oid&#39;: &#39;540379650&#39;, &#39;type&#39;: &#39;1&#39;, &#39;message&#39;: message, &#39;plat&#39;: &#39;1&#39;, &#39;jsonp&#39;: &#39;jsonp&#39;, &#39;csrf&#39;: &#39;2465bf0f43863f1f72da4437e8a3f7f7&#39; &#125; #data &#x3D; bytes(parse.urlencode(form_data), encoding&#x3D;&#39;utf-8&#39;) headers &#x3D; &#123; &quot;User-agent&quot;: &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;81.0.4044.129 Safari&#x2F;537.36&quot;, &quot;Host&quot;: &quot;api.bilibili.com&quot;, &quot;Content-Type&quot;: &quot;application&#x2F;x-www-form-urlencoded; charset&#x3D;UTF-8&quot;, &quot;Accept&quot;: &quot;application&#x2F;json, text&#x2F;javascript, *&#x2F;*; q&#x3D;0.01&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate, br&quot;, &quot;Accept-Language&quot;: &quot;zh-CN,zh;q&#x3D;0.9&quot;, &quot;Connection&quot;: &quot;keep-alive&quot;, &quot;Origin&quot;: &quot;https:&#x2F;&#x2F;www.bilibili.com&quot;, &quot;Referer&quot;: &quot;https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;BV1ii4y1t7Ri?from&#x3D;search&amp;seid&#x3D;13227842417220944957&quot;, &quot;Cookie&quot;: &quot;_uuid&#x3D;6545DCA1-06AD-4CC5-DAEE-66A47A079FB968566infoc; buvid3&#x3D;FB65770C-8357-41F6-B780-8B395463BE9253923infoc; sid&#x3D;lfvz8p4k; DedeUserID&#x3D;452320745; DedeUserID__ckMd5&#x3D;d305967df50bec7c; SESSDATA&#x3D;c722af0c%2C1602934275%2C4c34d*41; bili_jct&#x3D;2465bf0f43863f1f72da4437e8a3f7f7; CURRENT_FNVAL&#x3D;16; LIVE_BUVID&#x3D;AUTO1815873823108358; rpdid&#x3D;|(J|)R)|~R|J0J&#39;ul)~J)kJ|m; CURRENT_QUALITY&#x3D;64; bfe_id&#x3D;393becc67cde8e85697ff111d724b3c8; PVID&#x3D;1&quot; &#125; response &#x3D; requests.post(data&#x3D;data,headers&#x3D;headers,url&#x3D;url) response &#x3D; json.loads(response.text) print(response) #print(response[&#39;data&#39;][&#39;success_toast&#39;])#被黑名单了，屏蔽了，哈哈哈哈crawler() 实战三 3.11234567891011121314151617181920212223242526import requestsimport refrom lxml import etreedef crawler(): url &#x3D; &#39;http:&#x2F;&#x2F;top.baidu.com&#x2F;category?c&#x3D;513&amp;fr&#x3D;topbuzz_b1_c513&#39; headers &#x3D; &#123; &#39;User-Agent&#39;:&#39;Mozilla&#x2F;5.0 (Linux; Android 6.0; Nexus 5 Build&#x2F;MRA58N) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;81.0.4044.129 Mobile Safari&#x2F;537.36&#39; &#125; response &#x3D; requests.get(url&#x3D;url,headers&#x3D;headers) response.encoding &#x3D; response.apparent_encoding #print(response.encoding) html &#x3D; response.text #print(response.text) #pattern &#x3D; re.compile(&#39;&lt;td&gt;\\s&lt;class&quot;keyword&quot;&gt;\\s+&lt;a.*?&gt;(.*?)&lt;&#x2F;a&gt;&#39;) #data &#x3D; re.findall(pattern,response) html&#x3D; etree.HTML(html) #for i in range(1,10): result &#x3D; html.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;main&quot;]&#x2F;div[3]&#x2F;div[2]&#x2F;div&#x2F;div&#x2F;div&#x2F;ul&#x2F;li&#x2F;div[1]&#x2F;a[1]&#x2F;text()&#39;) index &#x3D; 0 print(&quot; 今日热点&quot;) for i in result: index +&#x3D; 1 print(index,&quot; ,&quot;,i)crawler() 3.212345678910111213141516171819202122232425262728293031323334# coding&#x3D;gbkimport requestsimport jsondef crawler(): message &#x3D; input(&#39;请输入你的评论:&#39;) url &#x3D; &#39; https:&#x2F;&#x2F;api.bilibili.com&#x2F;x&#x2F;v2&#x2F;reply&#x2F;add&#39; data &#x3D; &#123; &#39;oid&#39;: &#39;540379650&#39;, &#39;type&#39;: &#39;1&#39;, &#39;message&#39;: message, &#39;plat&#39;: &#39;1&#39;, &#39;jsonp&#39;: &#39;jsonp&#39;, &#39;csrf&#39;: &#39;2465bf0f43863f1f72da4437e8a3f7f7&#39; &#125; #data &#x3D; bytes(parse.urlencode(form_data), encoding&#x3D;&#39;utf-8&#39;) headers &#x3D; &#123; &quot;User-agent&quot;: &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;81.0.4044.129 Safari&#x2F;537.36&quot;, &quot;Host&quot;: &quot;api.bilibili.com&quot;, &quot;Content-Type&quot;: &quot;application&#x2F;x-www-form-urlencoded; charset&#x3D;UTF-8&quot;, &quot;Accept&quot;: &quot;application&#x2F;json, text&#x2F;javascript, *&#x2F;*; q&#x3D;0.01&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate, br&quot;, &quot;Accept-Language&quot;: &quot;zh-CN,zh;q&#x3D;0.9&quot;, &quot;Connection&quot;: &quot;keep-alive&quot;, &quot;Origin&quot;: &quot;https:&#x2F;&#x2F;www.bilibili.com&quot;, &quot;Referer&quot;: &quot;https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;BV1ii4y1t7Ri?from&#x3D;search&amp;seid&#x3D;13227842417220944957&quot;, &quot;Cookie&quot;: &quot;_uuid&#x3D;6545DCA1-06AD-4CC5-DAEE-66A47A079FB968566infoc; buvid3&#x3D;FB65770C-8357-41F6-B780-8B395463BE9253923infoc; sid&#x3D;lfvz8p4k; DedeUserID&#x3D;452320745; DedeUserID__ckMd5&#x3D;d305967df50bec7c; SESSDATA&#x3D;c722af0c%2C1602934275%2C4c34d*41; bili_jct&#x3D;2465bf0f43863f1f72da4437e8a3f7f7; CURRENT_FNVAL&#x3D;16; LIVE_BUVID&#x3D;AUTO1815873823108358; rpdid&#x3D;|(J|)R)|~R|J0J&#39;ul)~J)kJ|m; CURRENT_QUALITY&#x3D;64; bfe_id&#x3D;393becc67cde8e85697ff111d724b3c8; PVID&#x3D;1&quot; &#125; response &#x3D; requests.post(data&#x3D;data,headers&#x3D;headers,url&#x3D;url) response &#x3D; json.loads(response.text) print(response) #print(response[&#39;data&#39;][&#39;success_toast&#39;])#被黑名单了，屏蔽了，哈哈哈哈crawler() 3.312345678910111213141516171819202122232425# -*- coding: utf-8 -*-import requestsfrom lxml import etreeimport ioimport sysdef crawler(): url &#x3D; &quot;https:&#x2F;&#x2F;www.baidu.com&#x2F;s?wd&#x3D;ip&amp;rsv_spt&#x3D;1&amp;rsv_iqid&#x3D;0xfed2c8ed0035172d&amp;issp&#x3D;1&amp;f&#x3D;8&amp;rsv_bp&#x3D;1&amp;rsv_idx&#x3D;2&amp;ie&#x3D;utf-8&amp;tn&#x3D;78000241_9_hao_pg&amp;rsv_enter&#x3D;1&amp;rsv_dl&#x3D;tb&amp;rsv_sug3&#x3D;3&amp;rsv_sug1&#x3D;3&amp;rsv_sug7&#x3D;101&amp;rsv_sug2&#x3D;0&amp;rsv_btype&#x3D;i&amp;inputT&#x3D;1307&amp;rsv_sug4&#x3D;2576&quot; headers &#x3D; &#123; &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;81.0.4044.129 Safari&#x2F;537.36&#39;, &#39;Cookie&#39;:&#39;BAIDUID&#x3D;A73587ED4F3222B80C4FE16CB6B65DD7:FG&#x3D;1; BIDUPSID&#x3D;A73587ED4F3222B80C4FE16CB6B65DD7; PSTM&#x3D;1587136751; BD_UPN&#x3D;12314753; yjs_js_security_passport&#x3D;0e6646cc08e32d531de1788125bb23f41137a2c9_1588410443_js; BDUSS&#x3D;FBOUNaOUhFMDdwQjhyLTBIS3FVZElEbm85c2l3cnYwajNSVUVOZTFVVEJFTlZlRVFBQUFBJCQAAAAAAAAAAAEAAADiLOpIs8K80eL5tcTE0MXz09EAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMGDrV7Bg61eQU; BDPASSGATE&#x3D;IlPT2AEptyoA_yiU4VKP3kIN8efjSri4D4eGSyRpQFStfCaWmhH3BrUzWz0HSieXBDP6wZTXdMsDxXTqXlVXa_EqnBsZolpOaSaXzKGoucHtVM69-t5yILXoHUE2sA8PbRhL-3MEF2ZELkwvcgjchQZrchW8z3Ibhx0shE76cfbW16nyZ-r-m6qF1VlHU-usNPWx9486mCgdPpyBUuL2LTTtmU19QJ1R7Aq7is5qOQD7vSEzD2KmL3YhGG85Jppg0huP3OyLxRWlFSoZwp-tKCkYjkiY6qjxSUIS2Kbpz1NeO1K; H_WISE_SIDS&#x3D;144089_143435_142019_144427_145945_140632_146113_145870_144420_144134_145271_146538_146308_145303_144961_131247_144681_141261_144250_141941_127969_146037_146549_140594_142421_145876_131423_100805_145909_146001_145598_107315_146135_139909_146395_144966_142427_145608_140367_143665_144018_146054_145397_143860_145073_139913_110085; SE_LAUNCH&#x3D;5%3A26473974; MSA_WH&#x3D;774_922; MSA_PBT&#x3D;146; MSA_PHY_WH&#x3D;1548_1844; MSA_ZOOM&#x3D;1056; FC_MODEL&#x3D;0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_1588438456%7C1%230_0_0_0_0_0_1588438456%7C1%230__0_0_0_0_351_1588438456; wpr&#x3D;0; BDRCVFR[Y1-7gJ950Fn]&#x3D;OjjlczwSj8nXy4Grjf8mvqV; BD_HOME&#x3D;1; H_PS_PSSID&#x3D;; delPer&#x3D;0; BD_CK_SAM&#x3D;1; PSINO&#x3D;1; H_PS_645EC&#x3D;4b056V7e%2Fk2YRqGMTC3b46ni%2FRA6WwQji2etuSJlHsv%2FgYT9LZQGZVIvIqX7WOFcpwkmg5rQr5E; BDORZ&#x3D;FFFB88E999055A3F8A630C64834BD6D0; COOKIE_SESSION&#x3D;573255_0_1_0_2_1_1_0_0_1_2_0_0_0_3_0_1587895012_0_1588468264%7C2%230_0_1588468264%7C1; BDSVRTM&#x3D;0&#39; &#125; sys.stdout &#x3D; io.TextIOWrapper(sys.stdout.buffer, encoding&#x3D;&#39;gb18030&#39;) #解决UnicodeEncodeError: &#39;gbk&#39; codec can&#39;t encode character &#39;\\ue780&#39; in position 439546: illegal multibyte sequence response &#x3D; requests.get(url&#x3D;url,headers&#x3D;headers) response.encoding &#x3D; &#39;utf-8&#39; html &#x3D; etree.HTML(response.text) ip_result &#x3D; html.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;1&quot;]&#x2F;div&#x2F;div&#x2F;div&#x2F;table&#x2F;tr&#x2F;td&#x2F;span&#x2F;text()&#39;)[0] print(ip_result[:5],ip_result[6:len(ip_result)-1]) #print(etree.tostring(html, encoding&#x3D;&#39;utf-8&#39;).decode(&#39;utf-8&#39;)) #print(response.content.decode(&#39;utf-8&#39;))crawler() 实战四123456789101112131415161718192021222324252627282930313233343536373839404142434445import requestsfrom lxml import etreeclass Crawler(): def __init__(self): self.BASE_URL &#x3D; &quot;https:&#x2F;&#x2F;bing.ioliu.cn&quot; self.TOTAL_PAGE &#x3D; 124 self.headers &#x3D; &#123; &quot;user-agent&quot;: &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.149 Safari&#x2F;537.36&quot; &#125; def spider(self): session &#x3D; requests.Session() for index in range(1 ,self.TOTAL_PAGE+1): url &#x3D; self.BASE_URL + f&quot;&#x2F;?p&#x3D;&#123;index&#125;&quot; response &#x3D; session.get(url&#x3D;url, headers&#x3D;self.headers) yield response.content.decode(&quot;utf8&quot;) @staticmethod def parser(html): selector &#x3D; etree.HTML(html) url_list &#x3D; selector.xpath(&quot;&#x2F;&#x2F;div[@class&#x3D;&#39;item&#39;]&#x2F;&#x2F;div[@class&#x3D;&#39;options&#39;]&#x2F;a[2]&#x2F;@href&quot;) return url_list def save_picture(self, url_list): session &#x3D; requests.Session() index &#x3D; 0 for url in url_list: index +&#x3D; 1 file_name &#x3D; url.split(&quot;&#x2F;&quot;)[2][:-15] + &quot;.jpg&quot; picture_url &#x3D; self.BASE_URL + url response &#x3D; session.get(picture_url, headers&#x3D;self.headers) with open(file_name, &quot;wb&quot;) as file: file.write(response.content) print(&#39;photo&#39;,index,&#39;:save successfully&#39;) def run(self): html_gen &#x3D; self.spider() for html in html_gen: url_list &#x3D; self.parser(html) self.save_picture(url_list)if __name__ &#x3D;&#x3D; &#39;__main__&#39;: bing_spider &#x3D; Crawler() bing_spider.run()","categories":[],"tags":[]},{"title":"爬虫小作业/爬取必应壁纸","slug":"爬虫小作业/爬取必应壁纸","date":"2020-05-14T11:50:05.207Z","updated":"2020-05-14T11:51:57.259Z","comments":true,"path":"2020/05/14/爬虫小作业/爬取必应壁纸/","link":"","permalink":"http://yoursite.com/2020/05/14/%E7%88%AC%E8%99%AB%E5%B0%8F%E4%BD%9C%E4%B8%9A/%E7%88%AC%E5%8F%96%E5%BF%85%E5%BA%94%E5%A3%81%E7%BA%B8/","excerpt":"","text":"通过封装成一个类来抓却图片，首先初始化需要使用到的地址、请求头信息和抓取的图片总数123456def __init__(self): self.BASE_URL &#x3D; &quot;https:&#x2F;&#x2F;bing.ioliu.cn&quot; self.TOTAL_PAGE &#x3D; 124 self.headers &#x3D; &#123; &quot;user-agent&quot;: &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.149 Safari&#x2F;537.36&quot; &#125; 保存好缓存信息，并且获得每一张的图片地址123456def spider(self): session &#x3D; requests.Session() for index in range(1 ,self.TOTAL_PAGE+1): url &#x3D; self.BASE_URL + f&quot;&#x2F;?p&#x3D;&#123;index&#125;&quot; response &#x3D; session.get(url&#x3D;url, headers&#x3D;self.headers) yield response.content.decode(&quot;utf8&quot;) 利用xpath语法提取没出每一个图片地址12345@staticmethoddef parser(html): selector &#x3D; etree.HTML(html) url_list &#x3D; selector.xpath(&quot;&#x2F;&#x2F;div[@class&#x3D;&#39;item&#39;]&#x2F;&#x2F;div[@class&#x3D;&#39;options&#39;]&#x2F;a[2]&#x2F;@href&quot;) return url_list 以二进制的形式保存图片1234567891011def save_picture(self, url_list): session &#x3D; requests.Session() index &#x3D; 0 for url in url_list: index +&#x3D; 1 file_name &#x3D; url.split(&quot;&#x2F;&quot;)[2][:-15] + &quot;.jpg&quot; picture_url &#x3D; self.BASE_URL + url response &#x3D; session.get(picture_url, headers&#x3D;self.headers) with open(file_name, &quot;wb&quot;) as file: file.write(response.content) print(&#39;photo&#39;,index,&#39;:save successfully&#39;)","categories":[],"tags":[]},{"title":"爬虫笔记/爬虫基础","slug":"爬虫笔记/爬虫基础","date":"2020-05-14T11:50:05.190Z","updated":"2020-04-29T09:51:02.098Z","comments":true,"path":"2020/05/14/爬虫笔记/爬虫基础/","link":"","permalink":"http://yoursite.com/2020/05/14/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/","excerpt":"","text":"爬虫基础0.爬虫的基本组成 URL管理模块 HTML下载模块 HTML解析模块 数据存储模块 爬虫调度模块 0.1 其他信息的了解 概念：爬虫是一种按照一定的规则，自动抓取万维网信息的程序或脚本 robots:是否允许其他爬⾍(通⽤爬⾍即浏览器)爬取某些内容 爬虫不遵守robots 爬虫不是黑客，它只能爬去到用户所能访问的数据 1.HTTP原理 1.1 URL和URI URI（Unifor Resource Identifier）：统一资源标识符 URL（Universa Resource Locator）：统一资源定位符 URN（Universal Resource Name）：统一资源名称 URI = URL + URN 通常情况下，网页链接使用的是URL 1.2 超文本（hypertext） 网页源代码即是一系列HTML代码，网页源代码由超文本解析后，形成我们所看到的网页 1.3 HTTP和HTTPS URL的开头会有http或https ，这就是访问资源需要的协议类型（有其他协议类型，但普遍是这两种） HTTP（Hyper Text Transfer Protocol）：超文本传输协议。用于从网络传输超文本数据到本地浏览器的传送协议，它能保证高效而准确地传送超文本文档。 HTTPS（Hyper Text Transfer Protocol over Secure Socket Layer）：HTTP + SSL（安全套接层，作用：加密超文本数），即HTTP的安全版。效率弱于HTTTP。 凡是使用了 HTTPS 的网站，，都可以通过点击浏览器地址栏的锁头标志来查看网站认证之后的真实信息，也可以通过 CA 机构颁发的安全签章来查询。 注意： 1、某些网站虽然使用了 HTTPS 协议，但还是会被浏览器提示不安全，时浏览器就会提示“您的连接不是私密连接”，只是因为没有CA证书的认可，但认识SSL加密了的。 2、爬取这样的站点，就需要设置忽略证书的选项，否则会提示 SSL 链接错误。 1.4 HTTP的请求过程 在浏览器中输入一个URL ，回车之后便会在浏览器中观察到页面内容 实际上，这个过程是浏览器向网站所在的服务器发送了一个请求，网站服务器接收到这个请求后进行处理和解析，然后返回对应的响应，接着传回给浏览器 响应里包含了页面的源代码等内容，浏览器再对其进行解析，便将网页呈现了出来。 如果输入的是域名则还需通过DNS解析返回IP地址后再发送请求。 请求图解析 Name ：请求的名称，一般会将 RL 最后一部分内容当作名称 Status ：响应的状态码，这里显示为200代表响应是正常的通过状态码，我们可以判断发送了请求之后是得到了正常的响应 Type:请求的文梢类型 这里为document ，代表我们这次请求的是HTML文档，内容就是一些HTML代码 Initiator:请求源,用来标记请求是由哪个对象或进程发起的 Size:从服务器下载的文件和请求的资源大小 如果是从缓存中取得的资源，则该列会显示台om cache Time:发起请求到获取响应所用的总时间 Waterfall：网络请求的可视化瀑布流 点击name下条目可以看见具体的详细信息 一般会有四个部分（有时是三个，看具体情况）：General、Response Headers（相应头）、Request Headers（请求头）、query string params(查询的参数，不一定有) General： Request URL：请求的 URL Request Method：请求的方法 Status Code：响应状态码 Remote Address ：远程服务器的地址和端口 Referrer Policy ：Referrer 判别策略 query string params ：查询数据用的参数 响应头和请求头是重点，具体的内容在下面 1.5 请求 数据请求的过程（浏览器）：浏览器搜索域名-&gt;域名服务器DNS解析成IP地址返回到浏览器-&gt;浏览器通过IP地址链接到服务器，然后服务器在数据库中搜索并返回相应数据给浏览器 请求，由客户端向服务端发出，可以分为4部分：请求方法（ Request Method ）\\请求的网址( Request URL ）、请求头（ Request Headers )、请求体（ Request Body ) 1.请求方法： GET和POST(这两种常用) GET:请求页面，并返回页面内容 POST:大多用于提交表单或上传文件，数据包含在请求体中 GET和POST的区别： GET 请求中的参数包含在 URL 里面，数据可以在 URL 中看到，而POST 请求的 URL 会包含这些数据，数据都是通过表单形式传输的，会包含在请求体中 GET 请求提交的数据最多只有 1024 字节，而 POST 方式没有限制 URL = htψs://www. baidu.corn/s?wd=....此处？后面的都是参数，键值对的形式存在此处wd表示要搜索关键字 POST一般是提交表单或上传文件，比如需要登陆，使用POST，也可以使用GET，但是用户名和密码直接暴露出来了，所以一般用POST 其他请求方法： HEAD：获取报头、 PUT：从客户端向服务器传送的数据取代指定文梢中的内容、 DELETE：请求服务器删除指定的页面、 CONNECT：把服务器当作跳板，让服务器代替客户端防问其他网页、 TRACE：囚显服务器收到的请求，主要用于测试或诊断 2.请求的网址 请求的网址，即统 资惊定位符 URL ，它可以唯一确定我们想请求的资源 3.请求头 请求头，用来说明服务器要使用的附加信息，比较重要的信息有 Cookie Referer User-Agent等。 Accept ：请求报头域，用于指定客户端可接受哪些类型的信息 Accept-Language ：指定客户端可接受的语言类型 Accept-Encoding ：指定客户端可接受的内容编码 Host：用于指定请求资源的主机 IP 和端口号，其内容为请求 URL 的原始服务器或网关的位置 Cookie（cookies） ：是网站为了辨别用户进行会话跟踪而存储在用户本地的数据。主要功能是维持当前访问会话。Cookies 里有信息标识了我们所对应的服务器的会话，每次浏览器在请求该站点的页面时，都会在请求头中加上 Cookies 并将其发送给服务器，服务器通过 Cookies 识别出是我们自己，并且查出当前状态是登录状态，所以返回结果就是登录之后才能看到的网页内容。 Referer ：此内容用来标识这个请求是从哪个页面发过来的，服务器可以拿到这 信息并做相应的处理，如做来源统计、防盗链处理等 User-Agent ：简称 UA ，它是一个特殊的字符串头，可以使服务器识别客户使用的操作系统及版本 浏览器及版本等信息 在做爬虫时加上此信息，可以伪装为浏览器；如果不加，很可能会被识别为爬虫。 Content-Type ：也叫互联网媒体类型（ Internet Media Type ）或者 MIME 类型，在 HTTP协议消息头中，它用来表示具体请求中的媒体类型信息。例如， text/html 代表 HTML 格式，image/gif 代表 GIF 图片， app lication/json代表JSON 类型等。 4.请求体 请求体 般承载的内容是 POST 请求中的表单数据，而对于 GET 请求，请求体则为空 注意：Request Headers 指定 Content-Type application, x-www-form-urlencoded 只有设置Content-Type application/x-www-form-urlencoded ，才会以表单数据的形式提交 另外，我们也可以Content-Type 设置为 pplication/ison 来提交 JSON 数据，或者设置为 multi part/form-data上传文件 1.6 响应 响应，由服务端返回给客户端，可以分为：响应状态码（ Response Status Code ）、响应头( Response Headers ）和响应体（ Response Body ） 1.响应状态码：响应状态码表示服务器的响应状态 200表示正常响应 2.响应头包含了服务器对请求的应答信息 Date：标识响应产生的时间 Last-Modified：指定资源的最后修改时间 Content-Encoding：指定响应内容的编码 Server 包含务器的信息，比如名称、版本号等 Content-Type：文档类型 ，指定返回的数据类型是什么，如 text/ html代表返回 HTML 文档，application/x-javascript代表返回 JavaScript文件， image/jpeg 代表返回图片 Set Cookie：设置 Cookie响应头中的 Set Cookie告诉浏览器需要将此内容放在 Cookies中，下次请求携带Cookies请求 3.响应体 （图片有误，非响应体内容） 响应的正文数据都在应体中，比如请求网页时，它的响应体就HTML；请求一张图片时，响应体就是图片的二进 制数据，爬虫请求网页后，要解析的内容就是响应体 点击response，就可以看到网页源代码也就是响应体的内容，它是解的目标 2.网页的基础知识 2.1 网页的组成 网页可分为3大部分一一－HTML、CSS、JavaScript（HTML相当于骨架， JavaScript相当于肌肉、css相当当于皮肤，三者结合才能形成完善的网页） 1.HTML（Hype Text Markup Language）:超文本标记语言。网页包括文字、按钮、图片 视频等各种复杂元素，其基础构架就是HTML。 2.CSS（Cascading Style Sheets）：层叠样式表。让页面变得美观。 3.JavaScript（JS）：一种脚本语言。动态的交互信息，例如：下载进度条、提示框 轮播图等。 2.2 网页的结构 有待完善3.爬虫的基本原理 3.1 爬虫概述 1.获取网页 获取网页的源代码，源代码里包含了网页的部分有用信息 ，所以只要把源代码获取下来，就可以从中提取想要的信息了 Python中借助的库：urllib、requests 2.提取信息 利用正则表达式、XPath从抓取的源代码提取所需数据 3.保存数据 文件、数据库保存 4.自动化程序 让程序自动抓取数据 3.2 抓取数据 无论是HTML、JSON还是二进制数据，都可以爬取4.Cookies基本原理 Cookies 指某些网站为了辨别用户身份 进行会话跟踪而存储在用户本地终端上的数据 1.会话维持 成功登录某个网站时，服务器会告诉客户端设置哪些cookies 信息，在后续访问页面时客户端会把 Cookies 发送给服务器，服务器再找到对应的会话加以判断.如果会话中的某些设置登录状态的变量是有效的，那就证明用户处于登录状态，此时返回登录之后才可以查看的网页内容，浏览器再进行解析便可以看到了. 如果传给服务器的 Cookies 是无效的，或者会话已经过期了，我们将不能继续访问页面，此时可能会收到错误的响应或者跳转到登录页面重新登录 Cookies 和会话需要配合，一个处于客户端，一个处于服务端，二者共同协作，就实现了登录会话控制 2.属性结构 Name：Cookie的名称。一旦创建，该名称便不可更改 Value Cookie 的值。如果值为 Unicode 字符，需要为字符编码 如果值为二进制数据，则需要使用 BASE64 编码 Domain ：可以访问改Cookie的域名 Max Age：Cookie失效的时间， 单位为秒，也常和 Expires 一起使用，通过它可以计算有效时间 Max Age 果为正数 ，则cookie Max Age 秒之后失效 如果为负数，则关闭浏览器时 Cookie 即失效，浏览器也不会以任何形式保存该 Cookie Path:Cookie 的使用路径 如果设置为／path/ ，则只有路径为 /path/ 的页面可以访问该Cookie 如果设置为人，则本域名下的所有页面都可以访问该 Cookie Size：cookie的字段大小 HTTP 字段： Cookie的httponly属性，若属性为true ，则只有在HTTP头中会带有此Cookie 的信息，而不能通过 document.cookie 来访问此 Cookie Secure Cookie：是否仅被使用安全协议传输。安全协议有 HTTPS、SSL 等，在网络上传输数据之前先将数据加密。默认为 false Max Age或Expires字段的设置，一些持久化登录的网站把 Cookie 的有效时间和会话有效期设置得比较长，下次再访问页面时，仍然携带之前的 Cookie ，就可以直接保持登录状态 注意：服务器无法判读浏览器是否关闭，而是服务器设置了失效时间，根据用户是否超过了失效时间，若超过服务器会关闭会话（以节省储存空间）。 5.代理 5.1 基本原理 代理实际上指的就是代理服务器，英文叫作 proxy server。作用：是代理网络用户去取得网络信息。也就是相当于一个处在本机和服务器中的一个中介 5.2 代理的作用 突破 门身 IP 访问限制，访问一些平时不能访问的站点 访问一些单位或团体内部资源（局域网） 提高访问速度 隐藏丘实 IP，防止 向身的IP被封锁 5.3 爬虫代理 原因： 1.爬虫爬取速度过快，在爬取过程中可能遇到同一个IP访问过于频繁的问题，此时网站就会让我们输入验证码登录或者直接封锁IP 2.使用代理隐藏真实的 IP ，让服务器误以为是代理服务器在请求自己。这样便能更好的获取数据 5.4 代理的分类 1.协议划分： 暂时不想写 2.匿名程度划分： 透明代理 代理服务器将客户端的信息转发至目标访问对象，并没有完全隐藏客户端真实的身份。即服务器知道客户端使 用了代理IP，并且知道客户端的真实IP地址。 普通匿名代理 代理服务器用自己的IP代替了客户端的真实IP，但是告诉了目标访问对象这是代理访问。 高匿代理 代理服务器良好地伪装了客户端，不但用一个随机的IP代替了客户端的IP，也隐藏了代理信息，服务器不会觉到客户端是通过代理实现访问的，即用户仿佛就是直接使用代理服务器作为自己的客户端。","categories":[],"tags":[]},{"title":"爬虫笔记/正则表达式","slug":"爬虫笔记/正则表达式","date":"2020-05-14T11:50:05.189Z","updated":"2020-05-14T10:45:37.643Z","comments":true,"path":"2020/05/14/爬虫笔记/正则表达式/","link":"","permalink":"http://yoursite.com/2020/05/14/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","excerpt":"","text":"没啥好说的，留个网址查吧：https://tool.oschina.net/uploads/apidocs/jquery/regexp.html 基本的操作还可以查（正则表达式在线）","categories":[],"tags":[]},{"title":"爬虫笔记/XPath","slug":"爬虫笔记/XPath","date":"2020-05-14T11:50:05.187Z","updated":"2020-05-03T11:16:41.053Z","comments":true,"path":"2020/05/14/爬虫笔记/XPath/","link":"","permalink":"http://yoursite.com/2020/05/14/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/XPath/","excerpt":"","text":"XPath，全称XML Path Language，即XML路径语言，它是一门在XML文档中查找信息的语言。其功能强大，提供了非常简洁明了的路径选择表达式。XPath筛选后的结果返回为列表。 常用规则|表达式|描述||—|—||nodename| 此节点的所有子节点||/ |从当前节点选取直接子节点||// |从当前节点选取子孙节点||. |选取当前节点}|.. |选取当前节点的父节点||@ |选取属性|基本使用方法 123456789101112131415161718192021222324252627282930313233343536373839from lxml import etree### 把html文件保存到本地，命名testtext &#x3D; &quot;&quot;&quot;&lt;div&gt;&lt;ul&gt;&lt;li class&#x3D;&quot;item-0&quot;&gt;&lt;a href&#x3D;&quot;link1.html&quot;&gt;first item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;&lt;li class&#x3D;&quot;item-1&quot;&gt;&lt;a href&#x3D;&quot;link2.html&quot;&gt;second item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;&lt;li class&#x3D;&quot;item-inactive&quot;&gt;&lt;a href&#x3D;&quot;link3.html&quot;&gt;third item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;&lt;li class&#x3D;&quot;item-1&quot;&gt;&lt;a href&#x3D;&quot;link4.html&quot;&gt;fourth item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;&lt;li class&#x3D;&quot;item-0&quot;&gt;&lt;a href&#x3D;&quot;link5.html&quot;&gt;fifth item&lt;&#x2F;a&gt;&lt;&#x2F;ul&gt;&lt;&#x2F;div&gt;&quot;&quot;&quot;# 基本使用html &#x3D; etree.parse(&#39;.&#x2F;test.html&#39;, etree.HTMLParser())# 所有节点result &#x3D; html.xpath(&#39;&#x2F;&#x2F;*&#39;)# 子节点result &#x3D; html.xpath(&#39;&#x2F;&#x2F;li&#x2F;a&#39;)# 父节点result &#x3D; html.xpath(&#39;&#x2F;&#x2F;a[@href&#x3D;&quot;link4.html&quot;]&#x2F;parent::*&#x2F;@class&#39;)# 属性匹配result &#x3D; html.xpath(&#39;&#x2F;&#x2F;li[@class&#x3D;&quot;item-0&quot;]&#39;)# 文本获取result &#x3D; html.xpath(&#39;&#x2F;&#x2F;li[@class&#x3D;&quot;item-0&quot;]&#x2F;a&#x2F;text()&#39;)# 属性获取result &#x3D; html.xpath(&#39;&#x2F;&#x2F;li&#x2F;a&#x2F;@href)# 属性多值匹配result &#x3D; html.xpath(&#39;&#x2F;&#x2F;li[contains(@class, &quot;li&quot;)]&#x2F;a&#x2F;text()&#39;)# 多属性匹配result &#x3D; html.xpath(&#39;&#x2F;&#x2F;li[contains(@class, &quot;li&quot;) and @name&#x3D;&quot;item&quot;]&#x2F;a&#x2F;text()&#39;)# 按序选择result &#x3D; html.xpath(&#39;&#x2F;&#x2F;li[last()]&#x2F;a&#x2F;text()&#39;)# 节点轴选择result &#x3D; html.xpath(&#39;&#x2F;&#x2F;li[1]&#x2F;ancestor::*&#39;)# 拼接同级标签内容total &#x3D; html.xpath(&#39;concat(&#x2F;&#x2F;span[@class&#x3D;&quot;total&quot;]&#x2F;span&#x2F;text(),&#x2F;&#x2F;span[@class&#x3D;&quot;unit&quot;]&#x2F;span&#x2F;text())&#39;)","categories":[],"tags":[]},{"title":"爬虫笔记/urllib","slug":"爬虫笔记/urllib","date":"2020-05-14T11:50:05.186Z","updated":"2020-04-29T14:33:19.006Z","comments":true,"path":"2020/05/14/爬虫笔记/urllib/","link":"","permalink":"http://yoursite.com/2020/05/14/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/urllib/","excerpt":"","text":"二、urllib库简介 urllib是Python内置的一个HTTP请求库 包含4个模块：request,error,parse,robotparser1.urllib.request 模拟浏览器像服务器发送请求，需要使用的是urllib.request模块，其中最基础的请求方法就是 urlopen() 方法,默认为GET请求，传入data后为POST请求 1urllib.request.urlopen(url, data&#x3D;None, [timeout, ]*, cafile&#x3D;None, capath&#x3D;None,cadefault&#x3D;False,context&#x3D;None) url 可以是字符串，也可以是自己构造的Request对象 data 是需要发送给服务器的数据对象，如果没有则不用填写， 默认为None timeout 超时设置，是一个可选参数，传入超时时间后，如果在指定的时间内服务器没有响应则抛出 timeout 异常，基本上爬虫时都会用到，以避免服务器未响应 cafile 和 capath 代表 CA 证书和 CA 证书的路径。如果使用 HTTPS 则可能需要用到 context 参数必须是 ssl.SSLContext 类型，用来指定 SSL 设置 请求网页 123456import urllib.request &quot;&quot;&quot;导入模块&quot;&quot;&quot;url &#x3D; &quot;http:&#x2F;&#x2F;www.baidu.com&quot;response &#x3D; urllib.request.urlopen(url) #使用 urlopen 方法获取到的是一个 http.client.HTTPResponse 对象html &#x3D; response.read() #使用 read() 方法将response中的网页源代码读出来print(html.decode(&#39;utf-8&#39;)) #使用 decode() 方法将读取出来的网页源代码编码转换成 utf-8 编码 其他的一些操作： response.getcode()，获取状态码 response.readline()，以字节流 返回所有得数据 以列表格式保存 response.getheaders()，获取响应头 response.geturl()，获取url ------ 构建Request对象 urlopen() 可以支撑我们的一些简单的请求，但是请记住，一个没有请求头的爬虫是没有灵魂的。虽然不使用请求头也可以访问一些网页，但是这样的行为是直接告诉服务器“我是一个爬虫”，那么服务器可能就会拒绝程序的请求，因此我们需要进行伪装，这时候我们就需要去构造我们的HTTP请求体，一个Request对象。12345678910111213import urllib.requestdef crawler(): url &#x3D; &quot;http:&#x2F;&#x2F;www.maoyan.com&quot; headers &#x3D; &#123; &quot;User-agent&quot;: &quot;Mozilla&#x2F;5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit&#x2F;534.50 (KHTML, like Gecko) Version&#x2F;5.1 Safari&#x2F;534.50&quot; &#125; request &#x3D; urllib.request.Request(url&#x3D;url,headers&#x3D;headers) response &#x3D; urllib.request.urlopen(request) print(response.read().decode(&#39;utf-8&#39;))crawler() 2.urllib.parse 用于解析URL（python由ASCII编码，不支持中文，输入中文时必须解析url，否则无法运行） 解析方法一：调用urllib.parse.quote 12345678910111213141516import urllib.requestimport urllib.parseimport stringdef crawler(): basis_url &#x3D; &quot;https:&#x2F;&#x2F;www.baidu.com&#x2F;s?wd&#x3D;&quot; key_word &#x3D; &quot;中文&quot; url &#x3D; basis_url + key_word #通过拼接来确定搜索内容 encode_url &#x3D; urllib.parse.quote(key_word, safe&#x3D;string.printable) #key_word中出现了中文，通过urllib.parse.quote的方法来解析成python的可识别ASCII字符，其中 safe&#x3D;string.printable是必须要求填写的 response &#x3D; urllib.request.urlopen(encode_url) html &#x3D; response.read().decode(&quot;utf-8&quot;) print(html)crawler() 解析方法二：调用urllib.parse.urlencode()，支持多个参数解析,参数用字典封装起来 通过该方法会直接将字典格式转化成转化为字符串：’wd’:’穷哈’ -&gt; 调用方法后：wd = %E7%A9%B7%E5%93%88 1234567891011import urllib.requestimport urllib.parsedef crawler(): basis_url &#x3D; &quot;http:&#x2F;&#x2F;www.baidu.com&#x2F;s?&quot; params &#x3D; &#123;&quot;wd&quot;: &quot;中文&quot;,&quot;keyword&quot;: &quot;你好&quot;&#125; encode_url &#x3D; urllib.parse.urlencode(params) print(encode_url) url &#x3D; basis_url + encode_url print(url)crawler() data参数的传入 在请求某些网页时需要携带一些数据，我们就需要使用到 data 我们编写时将data编写为一个字典类型，使用时需要被转码成字节流，这就需要需要使用 urllib.parse.urlencode() 将字典转化为字符串，再使用 bytes() 转为字节流，最后使用 urlopen() 发起请求，请求是模拟用 POST 方式提交表单数据1234567891011121314import urllib.parseimport urllib.requestdef crawler(): url &#x3D; &quot;https:&#x2F;&#x2F;cn.bing.com&#x2F;ttranslatev3?sVertical&#x3D;1&amp;&amp;IG&#x3D;2B73CBCDC8F54EAFABF49389E29DC19A&amp;IID&#x3D;translator.5028.2&quot; form_data &#x3D; &#123; &quot;fromLang&quot;: &quot;auto-detect&quot;, &quot;text&quot;: &quot;爬虫&quot;, &quot;to&quot;: &quot;en&quot; &#125; data &#x3D; bytes(urllib.parse.urlencode(form_data), encoding&#x3D;&#39;utf8&#39;) response &#x3D; urllib.request.urlopen(url, data&#x3D;data) print(response.read().decode(&#39;utf-8&#39;))crawler() 代理IP的引入 系统的urlopen并没有添加代理的功能所以需要我们自定义这个功能，构建处理器hander和opener 原始的urlopen()方法已经够强大了，但是这并不能满足我们构建一个更加像浏览器的爬虫，所以这时候需要我们自己去构造我们的处理器（handler），然后通过OpenerDirector（opener）去使用处理器，urlopen就是一个Python为我们构造好的opener。 Handler 能处理请求（HTTP、HTTPS、FTP等）中的各种事情，它是通过 urllib.request.BaseHandler 这个类来实 现的。urllib.request.BaseHandler是所有的 Handler 的基类，其提供了最基本的Handler的方法 常见的Handler类： ProxyHandler ：为请求设置代理 HTTPCookieProcessor ：处理 HTTP 请求中的 Cookies HTTPPasswordMgr ：用于管理密码，它维护了用户名密码的表。 HTTPBasicAuthHandler ：用于登录认证，一般和 HTTPPasswordMgr 结合使用。 创建hander 1234567891011121314import urlib.requestdef myOpener(): url &#x3D; &quot;http:&#x2F;&#x2F;www.baidu.com&quot; handler &#x3D; urllib.request.HTTPHandler() #创建自己的hander opener&#x3D;urllib.request.build_opener(handler) #创建自己的oppener response &#x3D; opener.open(url) #用自己创建的opener调用open方法请求数据 data &#x3D; response.read().decode(&quot;utf-8&quot;) print(data)myOpener 代理IP的使用（普通） 123456789101112131415161718import urllib.requestdef spider(): url &#x3D; &quot;http:&#x2F;&#x2F;www.httpbin.org&#x2F;ip&quot; #proxy为代理IP proxy &#x3D; &#123; &quot;http&quot;: &quot;110.243.2.58:9999&quot;, &quot;http&quot;: &quot;117.69.150.100:9999&quot; &#125; #创建代理IP的hander proxy_handler &#x3D; urllib.request.ProxyHandler(proxy) #创建代理IP的opener opener &#x3D; urllib.request.build_opener(proxy_handler) #用代理ip去发送请求 response &#x3D; opener.open(url) data &#x3D; response.read().decode(&quot;utf-8&quot;) print(data) spider() 代理IP的使用（购买） 1234567891011121314151617181920import urllib.requestdef spider(): #1.添加用户名和密码 use_name &#x3D; &quot;name&quot; pwd &#x3D; &quot;123456&quot; proxy_money &#x3D; &quot;代理ip地址&quot; #2.创建密码管理器,添加用户名和密码 password_manager &#x3D; urllib.request.HTTPPasswordMgrWithDefaultRealm() #url 资源定位符 password_manager.add_password(None,proxy_money,use_name,pwd) #3.创建可以验证代理ip的处理器 handle_auth_proxy &#x3D; urllib.request.ProxyBasicAuthHandler(password_manager) #4.根据处理器创建opener opener_auth &#x3D; urllib.request.build_opener(handle_auth_proxy) #5.发送请求 response &#x3D; opener_auth.open(&quot;http:&#x2F;&#x2F;www.baidu.com&quot;) print(response.read())spider() 局域网爬取（auth认证） 和购买的IP一样需要输入用户名和密码，创建密码管理器和处理器123456789101112131415161718192021222324import urllib.requestdef local_area_network(): #1.添加用户名密码 user &#x3D; &quot;admin&quot; pwd &#x3D; &quot;123456&quot; local_area_network_url &#x3D; &quot;http:&#x2F;&#x2F;192....&quot; #2.创建密码管理器 pwd_manager &#x3D; urllib.request.HTTPPasswordMgrWithDefaultRealm() pwd_manager.add_password(None,nei_url,user,pwd) #创建认证处理器(requests) auth_handler &#x3D; urllib.request.HTTPBasicAuthHandler(pwd_manager) opener &#x3D; urllib.request.build_opener(auth_handler) response &#x3D; opener.open(local_area_network_url) print(response)local_area_network() ####cookie介绍 为什么要使用Cookie呢？ Cookie，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密）比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容是不允许的。那么我们可以利用Urllib库保存我们登录的Cookie，然后再抓取其他页面就达到目的了。 同样urlopen不能满足我们，我们还是需要构建opener 需要使用到的模块：http.cookiejar12345678910111213141516import http.cookiejarimport urllib.requestdef crawler(): url &#x3D; &quot;http:&#x2F;&#x2F;tieba.baidu.com&#x2F;&quot; headers &#x3D; &#123; &quot;User-agent&quot;: &quot;Mozilla&#x2F;5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit&#x2F;534.50 (KHTML, like Gecko) Version&#x2F;5.1 Safari&#x2F;534.50&quot;, &quot;Cookie&quot;: &quot;cookies&quot; &#125; request &#x3D; urllib.request.Request(url,headers&#x3D;headers) cookie &#x3D; http.cookiejar.CookieJar() handler &#x3D; urllib.request.HTTPCookieProcessor(cookie) opener &#x3D; urllib.request.build_opener(handler) response &#x3D; opener.open(requests)crawler() 3.urllib.robotparse 解析robots.txt文件 4.urllib.error 处理urllib.request抛出的异常类型","categories":[],"tags":[]},{"title":"爬虫笔记/reuquests库","slug":"爬虫笔记/reuquests库","date":"2020-05-14T11:50:05.184Z","updated":"2020-05-14T09:28:34.649Z","comments":true,"path":"2020/05/14/爬虫笔记/reuquests库/","link":"","permalink":"http://yoursite.com/2020/05/14/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/reuquests%E5%BA%93/","excerpt":"","text":"###reuquests库 衍生与urllib库，比urllib库更加强大和实用 属于第三方库，需要下载安装requests库12345678910111213import requestsurl &#x3D; &#39;http:&#x2F;&#x2F;www.baidu.com&#39;headers &#x3D; &#123; &#39;User-Agent&#39;:&#39;Mozilla&#x2F;5.0&#39;&#125;try: response &#x3D; requests.get(url, headers &#x3D; headers) response.raise_for_status() response.encoding &#x3D; r.apparent_encoding return response.textexcept: print(&#39;爬取失败&#39;) 实例引入12345678import requestsresponse &#x3D; requests.get(&#39;https:&#x2F;&#x2F;www.baidu.com&#x2F;&#39;)print(type(response))print(response.status_code)print(type(response.text))print(response.text)print(response.cookies) 各种请求方式1234567import requestsrequests.post(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;post&#39;)requests.put(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;put&#39;)requests.delete(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;delete&#39;)requests.head(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;get&#39;)requests.options(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;get&#39;) 带参数的get请求12345678import requestsdata &#x3D; &#123; &#39;name&#39;:&#39;gemey&#39;, &#39;age&#39;:22&#125;response &#x3D; requests.get(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;get&#39;, params&#x3D;data)print(response.text) 转换成json格式12345678import requestsimport jsonresponse &#x3D; requests.get(&#39;https:&#x2F;&#x2F;httpbin.org&#x2F;get&#39;)print(type(response.text))print(response.json())print(json.loads(response.text))print(type(response.json())) 获取二进制数据123456import requestsresponse &#x3D; requests.get(&#39;http:&#x2F;&#x2F;github.com&#x2F;favicon.ico&#39;)with open(&#39;favicon.ico&#39;,&#39;wb&#39;) as f: f.write(response.content) f.close() 添加headers1234567import requestsheaders &#x3D; &#123; &#39;User-Agent&#39;:&#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.132 Safari&#x2F;537.36&#39;&#125;response &#x3D; requests.get(&#39;https:&#x2F;&#x2F;www.zhihu.com&#x2F;explore&#39;, headers&#x3D;headers)print(response.text) 高级用法文件上传12345import requestsfiles &#x3D; &#123;&#39;file&#39;:open(&#39;favicon.ico&#39;, &#39;rb&#39;)&#125;response &#x3D; requests.post(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;post&#39;, files&#x3D;files)print(response.text) 获取cookies123456import requestsresponse &#x3D; requests.get(&#39;http:&#x2F;&#x2F;www.baidu.com&#39;)print(response.cookies)for key, value in response.cookies.items(): print(key + &#39;&#x3D;&#39; + value) 会话维持123456import requestss &#x3D; requests.Session()s.get(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;cookies&#x2F;set&#x2F;number&#x2F;123456789&#39;)response &#x3D; s.get(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;cookies&#39;)print(response.text) 异常处理1234567import requeststry: response &#x3D; requests.get(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;get&#39;, timeout&#x3D;0.1) print(response.status_code)except Exception as e: print(e) 图片下载保存路径和命名方法1234567891011121314151617181920212223242526import requestsimport osurl &#x3D; &#39;https:&#x2F;&#x2F;img10.360buyimg.com&#x2F;n5&#x2F;s75x75_jfs&#x2F;t1&#x2F;42272&#x2F;20&#x2F;10323&#x2F;135815&#x2F;5d3d6adfE63e2cb03&#x2F;7c966e6a71f02044.jpg&#39;headers &#x3D; &#123; &#39;User-Agent&#39;:&#39;Mozilla&#x2F;5.0&#39;&#125;root &#x3D; &#39;D:&#x2F;&#x2F;images&#x2F;&#x2F;&#39;path &#x3D; root + url.split(&#39;&#x2F;&#39;)[-1]try: if not os.path.exists(root): os.mkdir(root) if not os.path.exists(path): response &#x3D; requests.get(url, headers&#x3D;headers) response.raise_for_status() response.encoding &#x3D; response.apparent_encoding with open(path, &#39;wb&#39;) as f: f.write(response.content) f.close() print(&#39;文件下载完成&#39;) else: print(&#39;文件已存在&#39;)except: print(&#39;爬取失败&#39;)打印出现中文乱码 以网页编码格式解析1response.encoding &#x3D; response.apparrent_code 先返回二进制格式，再转码成utf-81response.content.decode(&#39;utf-8&#39;)","categories":[],"tags":[]}],"categories":[],"tags":[]}